# typed: strong

module Deeprails
  module Resources
    class Monitor
      # Use this endpoint to create a new monitor to evaluate model inputs and outputs
      # using guardrails
      sig do
        params(
          guardrail_metrics:
            T::Array[Deeprails::MonitorCreateParams::GuardrailMetric::OrSymbol],
          name: String,
          description: String,
          file_search: T::Array[String],
          web_search: T::Boolean,
          request_options: Deeprails::RequestOptions::OrHash
        ).returns(Deeprails::MonitorCreateResponse)
      end
      def create(
        # An array of guardrail metrics that the model input and output pair will be
        # evaluated on. For non-enterprise users, these will be limited to `correctness`,
        # `completeness`, `instruction_adherence`, `context_adherence`,
        # `ground_truth_adherence`, and/or `comprehensive_safety`.
        guardrail_metrics:,
        # Name of the new monitor.
        name:,
        # Description of the new monitor.
        description: nil,
        # An array of file IDs to search in the monitor's evaluations. Files must be
        # uploaded via the DeepRails API first.
        file_search: nil,
        # Whether to enable web search for this monitor's evaluations. Defaults to false.
        web_search: nil,
        request_options: {}
      )
      end

      # Use this endpoint to retrieve the details and evaluations associated with a
      # specific monitor
      sig do
        params(
          monitor_id: String,
          limit: Integer,
          request_options: Deeprails::RequestOptions::OrHash
        ).returns(Deeprails::MonitorDetailResponse)
      end
      def retrieve(
        # The ID of the monitor to retrieve.
        monitor_id,
        # Limit the number of returned evaluations associated with this monitor. Defaults
        # to 10.
        limit: nil,
        request_options: {}
      )
      end

      # Use this endpoint to update the name, description, or status of an existing
      # monitor
      sig do
        params(
          monitor_id: String,
          description: String,
          name: String,
          status: Deeprails::MonitorUpdateParams::Status::OrSymbol,
          request_options: Deeprails::RequestOptions::OrHash
        ).returns(Deeprails::MonitorUpdateResponse)
      end
      def update(
        # The ID of the monitor to edit.
        monitor_id,
        # Description of the monitor.
        description: nil,
        # Name of the monitor.
        name: nil,
        # Status of the monitor. Can be `active` or `inactive`. Inactive monitors no
        # longer record and evaluate events.
        status: nil,
        request_options: {}
      )
      end

      # Use this endpoint to retrieve the details of a specific monitor event
      sig do
        params(
          event_id: String,
          monitor_id: String,
          request_options: Deeprails::RequestOptions::OrHash
        ).returns(Deeprails::MonitorEventDetailResponse)
      end
      def retrieve_event(
        # The ID of the requested monitor event.
        event_id,
        # The ID of the monitor associated with this event.
        monitor_id:,
        request_options: {}
      )
      end

      # Use this endpoint to submit a model input and output pair to a monitor for
      # evaluation
      sig do
        params(
          monitor_id: String,
          model_input: Deeprails::MonitorSubmitEventParams::ModelInput::OrHash,
          model_output: String,
          nametag: String,
          run_mode: Deeprails::MonitorSubmitEventParams::RunMode::OrSymbol,
          request_options: Deeprails::RequestOptions::OrHash
        ).returns(Deeprails::MonitorEventResponse)
      end
      def submit_event(
        # The ID of the monitor associated with this event.
        monitor_id,
        # A dictionary of inputs sent to the LLM to generate output. The dictionary must
        # contain at least a `user_prompt` field or a `system_prompt` field. For
        # ground_truth_adherence guardrail metric, `ground_truth` should be provided.
        model_input:,
        # Output generated by the LLM to be evaluated.
        model_output:,
        # An optional, user-defined tag for the event.
        nametag: nil,
        # Run mode for the monitor event. The run mode allows the user to optimize for
        # speed, accuracy, and cost by determining which models are used to evaluate the
        # event. Available run modes include `precision_plus`, `precision`, `smart`, and
        # `economy`. Defaults to `smart`.
        run_mode: nil,
        request_options: {}
      )
      end

      # @api private
      sig { params(client: Deeprails::Client).returns(T.attached_class) }
      def self.new(client:)
      end
    end
  end
end
